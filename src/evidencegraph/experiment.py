"""
@author: Andreas Peldszus
"""


from collections import defaultdict
from datetime import datetime

from numpy import mean

from evidencegraph.classifiers import EvidenceGraphClassifier
from evidencegraph.utils import hash_of_featureset
from evidencegraph.utils import set_random_seed


modelpath = "data/models/"
default_random_seed = 0


def run_experiment_condition(
    in_corpus,
    out_corpus,
    folds,
    features,
    params,
    condition_name,
    modelpath=modelpath,
):
    set_random_seed(params.get("seed", default_random_seed))
    maF1s = defaultdict(list)
    miF1s = defaultdict(list)
    predictions = defaultdict(dict)
    decisions = defaultdict(dict)

    for train_tids, test_tids, i in folds:
        print("[{}] Iteration: {}\t".format(datetime.now(), i))
        ensemble_basename = condition_name.split("|")[0]
        ensemble_name = "{}__{}__{}".format(
            ensemble_basename, hash_of_featureset(features.feature_set), i
        )
        clf = EvidenceGraphClassifier(
            features.feature_function_segments,
            features.feature_function_segmentpairs,
            **params,
        )
        train_txt = [g for t, g in in_corpus.items() if t in train_tids]
        train_arg = [g for t, g in out_corpus.items() if t in train_tids]
        try:
            # load ensemble of pretrained base classifiers
            clf.load(modelpath + ensemble_name)
            if params["optimize_weighting"]:
                # and train metaclassifier (if desired)
                clf.train_metaclassifier(train_txt, train_arg)
        except RuntimeError:
            # train ensemble
            clf.train(train_txt, train_arg)
            clf.save(modelpath + ensemble_name)

        # test
        test_txt = [g for t, g in in_corpus.items() if t in test_tids]
        test_arg = [g for t, g in out_corpus.items() if t in test_tids]
        score_msg = ""
        for level, base_classifier in clf.ensemble.items():
            maF1, miF1 = base_classifier.test(test_txt, test_arg)
            maF1s[level].append(maF1)
            miF1s[level].append(miF1)
            score_msg += "{}: {:.3f}\t".format(level, maF1)
        decoded_scores = []
        for t in test_tids:
            mst = clf.predict(in_corpus[t])
            decoded_scores.append(clf.score(mst, out_corpus[t]))
            predictions[i][t] = mst.get_triples()
            decisions[i][t] = clf.predict_decisions(in_corpus[t])
        score_msg += "decoded: {:.3f}\t".format(mean(decoded_scores))
        print(score_msg)

    print("Average macro and micro F1:")
    for level in maF1s:
        avg_maF1 = mean(maF1s[level])
        avg_miF1 = mean(miF1s[level])
        print(level, avg_maF1, avg_miF1)

    return predictions, decisions
